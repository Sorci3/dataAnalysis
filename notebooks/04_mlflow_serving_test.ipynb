{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2cc075",
   "metadata": {},
   "source": [
    "# 04 - Test du serving MLflow\n",
    "\n",
    "Ce notebook valide le modèle servi via Docker/MLflow.\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "1. Construire l'image : `docker build -t credit-scoring-model .`\n",
    "2. Lancer le conteneur : `docker run -p 1234:1234 credit-scoring-model`\n",
    "3. Vérifier que le serveur répond : `curl http://localhost:1234/health`\n",
    "\n",
    "## Notes importantes\n",
    "\n",
    "- Le dataset de test est déjà préparé (même format que lors de l'entraînement)\n",
    "- Seule conversion nécessaire : les colonnes lues comme `object` par pandas doivent être converties en numériques\n",
    "- Les NaN sont conservés (LightGBM les supporte nativement)\n",
    "- Aucun autre retraitement n'est nécessaire\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e50d235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.model_utils import clean_feature_names\n",
    "from src.metrics import custom_business_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f9a30",
   "metadata": {},
   "source": [
    "## 2. Chargement du (Données de Test)\n",
    "\n",
    "Pour évaluer le modèle en conditions réelles tout en pouvant calculer des métriques, nous utilisons le **sample de 10 000 lignes** isolé lors de l'étape de préparation des données.\n",
    "\n",
    "* **`test_sample_features.csv`** : Les caractéristiques clients (ce qu'on envoie à l'API).\n",
    "* **`test_sample_labels.csv`** : La vérité terrain (TARGET), gardée en local pour vérifier la réponse de l'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données chargées : 10000 échantillons, 1845 features\n"
     ]
    }
   ],
   "source": [
    "URL = \"http://localhost:1234/invocations\"\n",
    "FEATURES_PATH = \"../datasets/final/test_sample_features.csv\"\n",
    "LABELS_PATH = \"../datasets/final/test_sample_labels.csv\"\n",
    "IDS_PATH = \"../datasets/final/test_sample_ids.csv\"\n",
    "SEUIL_OPTIMAL = 0.5\n",
    "\n",
    "df_features = pd.read_csv(FEATURES_PATH, keep_default_na=True, na_values=['', 'NULL', 'N/A', 'null', 'None'])\n",
    "df_labels = pd.read_csv(LABELS_PATH)\n",
    "df_ids = pd.read_csv(IDS_PATH)\n",
    "\n",
    "print(f\"Données chargées : {df_features.shape[0]} échantillons, {df_features.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f753ee",
   "metadata": {},
   "source": [
    "## 3. Prétraitement et Sérialisation JSON\n",
    "\n",
    "Le modèle hébergé dans Docker (via MLflow) est strict sur les formats d'entrée. Nous devons reproduire les transformations effectuées avant l'entraînement :\n",
    "\n",
    "1.  **Nettoyage des noms de colonnes :** Suppression des caractères spéciaux (via `clean_feature_names`).\n",
    "2.  **Suppression des IDs :** Le modèle ne prend pas `SK_ID_CURR` en entrée.\n",
    "3.  **Typage fort :** Conversion explicite de toutes les données en numérique (`float64`) pour éviter les erreurs d'interprétation du JSON par le serveur (erreur `arrays of bytes/strings`).\n",
    "4.  **Formatage :** Conversion en JSON au format `split` (orienté colonnes) pour l'envoi HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = clean_feature_names(df_features)\n",
    "\n",
    "cols_to_drop = ['SK_ID_CURR', 'index', 'Unnamed: 0'] \n",
    "X_final = X_clean.drop(columns=[c for c in cols_to_drop if c in X_clean.columns])\n",
    "\n",
    "for col in X_final.columns:\n",
    "    if X_final[col].dtype == 'object':\n",
    "        X_final[col] = pd.to_numeric(X_final[col], errors='coerce')\n",
    "    elif X_final[col].dtype == 'bool':\n",
    "        X_final[col] = X_final[col].astype(int)\n",
    "    if X_final[col].dtype in [np.int64, np.int32, np.int16, np.int8]:\n",
    "        X_final[col] = X_final[col].astype(np.float64)\n",
    "\n",
    "X_final = X_final.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "json_payload = json.loads(X_final.to_json(orient=\"split\", double_precision=15))\n",
    "payload = {\"dataframe_split\": json_payload}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aee42f",
   "metadata": {},
   "source": [
    "## 4. Appel API et Analyse de la Performance\n",
    "\n",
    "Nous simulons ici l'envoi en masse de dossiers de crédit au serveur.\n",
    "\n",
    "1.  Envoi d'une requête `POST` à `http://localhost:1234/invocations`.\n",
    "2.  Récupération des probabilités de défaut renvoyées par le modèle Docker.\n",
    "3.  Comparaison avec les vrais labels (`y_true`) pour calculer :\n",
    "    * **L'AUC :** Performance globale du modèle.\n",
    "    * **Le Coût Métier :** Impact financier (10 * Faux Négatifs + 1 * Faux Positifs).\n",
    "    * **La Matrice de Confusion :** Détail des erreurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2cfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats du serving\n",
      "\n",
      "AUC Score : 0.7160\n",
      "Coût Métier Total : 4,880\n",
      "\n",
      "Matrice de Confusion :\n",
      "   - Vrais Négatifs : 6,773\n",
      "   - Vrais Positifs : 561\n",
      "   - Faux Positifs  : 2,420  (Coût : 2,420)\n",
      "   - Faux Négatifs  : 246  (Coût : 2,460)\n",
      "\n",
      "Rapport de Classification :\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   Bons payeurs       0.96      0.74      0.84      9193\n",
      "Mauvais Payeurs       0.19      0.70      0.30       807\n",
      "\n",
      "       accuracy                           0.73     10000\n",
      "      macro avg       0.58      0.72      0.57     10000\n",
      "   weighted avg       0.90      0.73      0.79     10000\n",
      "\n",
      "Exemples de décisions pour 5 clients\n",
      "ID Client    | Score      | Décision     | Réalité    | Check\n",
      "374942       | 0.0000     | ACCORDÉ      | OK         | 1\n",
      "449334       | 0.0000     | ACCORDÉ      | OK         | 1\n",
      "308306       | 0.0000     | ACCORDÉ      | OK         | 1\n",
      "431049       | 0.0000     | ACCORDÉ      | OK         | 1\n",
      "208379       | 0.0000     | ACCORDÉ      | OK         | 1\n"
     ]
    }
   ],
   "source": [
    "response = requests.post(URL, json=payload, timeout=30)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "    predictions = response_data.get('predictions', response_data) if isinstance(response_data, dict) else response_data\n",
    "    \n",
    "    y_prob = []\n",
    "    for p in predictions:\n",
    "        if isinstance(p, list):\n",
    "            score = float(p[1]) if len(p) > 1 else float(p[0]) \n",
    "        else:\n",
    "            score = float(p)\n",
    "        y_prob.append(score)\n",
    "    \n",
    "    y_prob = np.array(y_prob)\n",
    "    y_true = df_labels['TARGET'].values\n",
    "    \n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    y_pred_bin = (y_prob >= SEUIL_OPTIMAL).astype(int)\n",
    "    cout_total = custom_business_cost(y_true, y_pred_bin)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_bin).ravel()\n",
    "    \n",
    "    print(\"Résultats du serving\")\n",
    "    print(f\"\\nAUC Score : {auc_score:.4f}\")\n",
    "    print(f\"Coût Métier Total : {cout_total:,}\")\n",
    "    print(f\"\\nMatrice de Confusion :\")\n",
    "    print(f\"   - Vrais Négatifs : {tn:,}\")\n",
    "    print(f\"   - Vrais Positifs : {tp:,}\")\n",
    "    print(f\"   - Faux Positifs  : {fp:,}  (Coût : {fp:,})\")\n",
    "    print(f\"   - Faux Négatifs  : {fn:,}  (Coût : {fn*10:,})\")\n",
    "    print(f\"\\nRapport de Classification :\")\n",
    "    print(classification_report(y_true, y_pred_bin, target_names=['Bons payeurs', 'Mauvais Payeurs']))\n",
    "    \n",
    "    print(\"Exemples de décisions pour 5 clients\")\n",
    "    print(f\"{'ID Client':<12} | {'Score':<10} | {'Décision':<12} | {'Réalité':<10} | {'Check'}\")\n",
    "    ids = df_ids['SK_ID_CURR'].values\n",
    "    for i in range(min(5, len(ids))):\n",
    "        dec = \"REFUSÉ\" if y_pred_bin[i] == 1 else \"ACCORDÉ\"\n",
    "        real = \"DÉFAUT\" if y_true[i] == 1 else \"OK\"\n",
    "        check = \"1\" if y_pred_bin[i] == y_true[i] else \"0\"\n",
    "        print(f\"{ids[i]:<12} | {y_prob[i]:.4f}     | {dec:<12} | {real:<10} | {check}\")\n",
    "else:\n",
    "    print(f\"Erreur API ({response.status_code}) : {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab43cd9",
   "metadata": {},
   "source": [
    "# Analyse\n",
    "**AUC**: On se retrouve avec un AUC bien plus faible qu'à l'entrainement et la validation, probablement car notre jeu de test est insuffisant. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
